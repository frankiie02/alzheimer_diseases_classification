{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install numpy keras SimpleITK scikit-learn deap h5py"
      ],
      "metadata": {
        "id": "z863r1r0eM4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow-gpu"
      ],
      "metadata": {
        "id": "QcW1VjU2eM1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "id": "BQeOtLkbeMx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install opencv-python"
      ],
      "metadata": {
        "id": "EYXdghgKeMuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from deap import base, creator, tools, algorithms"
      ],
      "metadata": {
        "id": "FmtoKX3xeMq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FcLAlO3GeMSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xphO0JS0eJvQ"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GlobalAveragePooling3D, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "from deap import base, creator, tools, algorithms\n",
        "import gc  # Import the garbage collection module\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define 3D data dimensions\n",
        "width, height, depth = (224, 224, 64)\n",
        "\n",
        "# Data paths\n",
        "data_folder = \"/content/drive/MyDrive/Normalization_output_file2\"\n",
        "output_folder = \"/content/drive/MyDrive/Models\"\n",
        "num_epochs = 20\n",
        "batch_size = 1\n",
        "\n",
        "# Mapping class names to labels\n",
        "class_name_to_label = {'VeryMildDemented': 0, 'MildDemented': 1, 'ModerateDemented': 2, 'NonDemented': 3}\n",
        "\n",
        "# Load and preprocess data\n",
        "data, labels = [], []\n",
        "for class_folder in os.listdir(data_folder):\n",
        "    if os.path.isdir(os.path.join(data_folder, class_folder)):\n",
        "        class_label = class_name_to_label.get(class_folder, -1)\n",
        "        if class_label != -1:\n",
        "            for file_name in os.listdir(os.path.join(data_folder, class_folder)):\n",
        "                if file_name.endswith(\".npy\"):\n",
        "                    data.append(np.load(os.path.join(data_folder, class_folder, file_name)))\n",
        "                    labels.append(class_label)\n",
        "\n",
        "data = np.array(data)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Train-validation split\n",
        "train_data, val_data, train_labels, val_labels = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define data augmentation generator\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\",\n",
        "    rescale=1.0 / 255\n",
        ")\n",
        "\n",
        "# Create the Xception-based 3D model\n",
        "num_classes = len(np.unique(train_labels))\n",
        "\n",
        "def create_model(learning_rate):\n",
        "    input_shape = (width, height, depth, 1 if len(train_data.shape) == 4 else train_data.shape[4])\n",
        "    base_model = tf.keras.applications.Xception(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    model = Sequential([\n",
        "        base_model,\n",
        "        GlobalAveragePooling3D(),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    optimizer = Adam(lr=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Distributed training strategy\n",
        "strategy = tf.distribute.experimental.CentralStorageStrategy()\n",
        "\n",
        "# Define data input pipelines for distributed training\n",
        "with strategy.scope():\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels)).batch(batch_size)\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_labels)).batch(batch_size)\n",
        "\n",
        "# Learning rate schedule and early stopping\n",
        "def lr_schedule(epoch):\n",
        "    return 0.001 * (0.1 ** int(epoch / 10))\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Define a function to evaluate the model's fitness\n",
        "def evaluate_model(individual):\n",
        "    learning_rate = individual[0]\n",
        "    model = create_model(learning_rate)\n",
        "\n",
        "    # Training\n",
        "    model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset, callbacks=[LearningRateScheduler(lr_schedule), early_stopping], verbose=0)\n",
        "\n",
        "    # Evaluation\n",
        "    _, val_accuracy = model.evaluate(val_dataset, verbose=0)\n",
        "\n",
        "    # Clear unnecessary variables\n",
        "    del model\n",
        "    gc.collect()  # Manually trigger garbage collection to clear memory\n",
        "\n",
        "    # Minimize the negative validation accuracy\n",
        "    return -val_accuracy\n",
        "\n",
        "# Clear unnecessary variables\n",
        "del data\n",
        "del labels\n",
        "del train_data\n",
        "del val_data\n",
        "\n",
        "# Create the DEAP toolbox\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"attr_float\", np.random.uniform, 1e-5, 1e-3)  # Learning rate range\n",
        "toolbox.register(\"individual\", tools.initCycle, creator.Individual, (toolbox.attr_float,), n=1)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "# Define the genetic operators\n",
        "toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
        "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.1, indpb=0.1)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "\n",
        "# Create an initial population of 10 individuals\n",
        "population = toolbox.population(n=10)\n",
        "\n",
        "# Create the hall of fame to store the best individual\n",
        "hall_of_fame = tools.HallOfFame(1)\n",
        "\n",
        "# Create the statistics object\n",
        "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "stats.register(\"max\", np.max)\n",
        "stats.register(\"avg\", np.mean)\n",
        "\n",
        "# Run the genetic algorithm\n",
        "NGEN = 10  # Number of generations\n",
        "CXPB = 0.7  # Crossover probability\n",
        "MUTPB = 0.2  # Mutation probability\n",
        "\n",
        "# Create a logbook to keep track of the evolution\n",
        "logbook = tools.Logbook()\n",
        "logbook.header = [\"gen\", \"evals\", \"max\", \"avg\"]\n",
        "\n",
        "for gen in range(NGEN):\n",
        "    # Evaluate the entire population\n",
        "    fitnesses = map(evaluate_model, population)\n",
        "    for ind, fit in zip(population, fitnesses):\n",
        "        ind.fitness.values = fit\n",
        "\n",
        "    # Update the hall of fame with the best individual\n",
        "    hall_of_fame.update(population)\n",
        "\n",
        "    # Clone the population\n",
        "    offspring = list(map(toolbox.clone, population))\n",
        "\n",
        "    # Apply crossover and mutation on the offspring\n",
        "    for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
        "        if np.random.rand() < CXPB:\n",
        "            toolbox.mate(child1, child2)\n",
        "            del child1.fitness.values\n",
        "            del child2.fitness.values\n",
        "\n",
        "    for mutant in offspring:\n",
        "        if np.random.rand() < MUTPB:\n",
        "            toolbox.mutate(mutant)\n",
        "            del mutant.fitness.values\n",
        "\n",
        "    # Evaluate the offspring\n",
        "    fitnesses = map(evaluate_model, offspring)\n",
        "    for ind, fit in zip(offspring, fitnesses):\n",
        "        ind.fitness.values = fit\n",
        "\n",
        "    # Replace the old population by the offspring\n",
        "    population[:] = offspring\n",
        "\n",
        "    # Record statistics\n",
        "    record = stats.compile(population)\n",
        "    logbook.record(gen=gen, evals=len(population), **record)\n",
        "    print(logbook.stream)\n",
        "\n",
        "# Get the best individual and its fitness value from the hall of fame\n",
        "best_ind = hall_of_fame[0]\n",
        "best_fitness = best_ind.fitness.values[0]\n",
        "print(\"Best Individual:\")\n",
        "print(\"Learning Rate =\", best_ind[0])\n",
        "print(\"Best Fitness =\", best_fitness)\n",
        "\n",
        "# Use the best learning rate to create and train the final model\n",
        "best_learning_rate = best_ind[0]\n",
        "with strategy.scope():\n",
        "    final_model = create_model(best_learning_rate)\n",
        "\n",
        "# Training\n",
        "final_model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset, callbacks=[LearningRateScheduler(lr_schedule), early_stopping])\n",
        "\n",
        "# Evaluation\n",
        "val_loss, val_accuracy = final_model.evaluate(val_dataset, verbose=0)\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Save the final model\n",
        "final_model.save(os.path.join(output_folder, \"Xception_model_3d.h5\"))\n",
        "\n",
        "# Clear unnecessary variables\n",
        "del data\n",
        "del labels\n",
        "del train_data\n",
        "del val_data\n"
      ]
    }
  ]
}