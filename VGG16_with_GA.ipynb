{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n"
      ],
      "metadata": {
        "id": "ouAK_fqpBPQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n"
      ],
      "metadata": {
        "id": "dQaMZT6PI4pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!conda install -c anaconda tensorflow-gpu\n",
        "\n"
      ],
      "metadata": {
        "id": "Blz62M7qI_s5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip cache purge\n",
        "!pip install tensorflow-gpu\n"
      ],
      "metadata": {
        "id": "lwt_IPJTOwJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "metadata": {
        "id": "8hUiHlBgJFFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n"
      ],
      "metadata": {
        "id": "xX9CYOvXO8__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deap"
      ],
      "metadata": {
        "id": "elkjQpiaBUFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install opencv-python"
      ],
      "metadata": {
        "id": "WsKB2EAvBk1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from deap import base, creator, tools, algorithms"
      ],
      "metadata": {
        "id": "VlSt2DvEBrWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e8dFhtEAmJf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16  # Import VGG16\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GlobalAveragePooling3D, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive (this will prompt you to authorize access)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the dimensions of your 3D data\n",
        "width, height, depth = (224, 224, 64)  # Adjust dimensions as needed\n",
        "\n",
        "# Load and preprocess 3D data from Google Drive\n",
        "data_folder = \"/content/drive/MyDrive/Normalization_output_file2\"  # Replace with your Google Drive folder path\n",
        "output_folder = \"/content/drive/MyDrive/Models\"  # Replace with the path to save trained models on Google Drive\n",
        "num_epochs = 20  # Adjust the number of epochs as needed\n",
        "batch_size = 1  # Adjust batch size as needed\n",
        "\n",
        "# Create a mapping from class names to numeric labels\n",
        "class_name_to_label = {\n",
        "    'VeryMildDemented': 0,\n",
        "    'MildDemented': 1,\n",
        "    'ModerateDemented': 2,\n",
        "    'NonDemented': 3\n",
        "}\n",
        "\n",
        "# Initialize empty lists to store data and labels\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "# Load data from the subfolders of your data folder\n",
        "for class_folder in os.listdir(data_folder):\n",
        "    if os.path.isdir(os.path.join(data_folder, class_folder)):\n",
        "        class_path = os.path.join(data_folder, class_folder)\n",
        "        class_label = class_name_to_label.get(class_folder, -1)  # Get the numeric label from the mapping\n",
        "\n",
        "        if class_label != -1:\n",
        "            for file_name in os.listdir(class_path):\n",
        "                if file_name.endswith(\".npy\"):\n",
        "                    file_path = os.path.join(class_path, file_name)\n",
        "                    image_data = np.load(file_path)\n",
        "                    # Check the number of channels in the image\n",
        "                    num_channels = image_data.shape[2] if len(image_data.shape) > 2 else 1\n",
        "                    data.append(image_data)\n",
        "                    labels.append(class_label)\n",
        "\n",
        "# Convert data and labels to NumPy arrays\n",
        "data = np.array(data)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Split the data into training and validation sets (80% - 20%)\n",
        "train_data, val_data, train_labels, val_labels = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define data generators with data augmentation for 3D volumes\n",
        "def data_generator(data, labels, batch_size, train=True):\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode=\"nearest\",\n",
        "        rescale=1.0 / 255\n",
        "    )\n",
        "\n",
        "    while True:\n",
        "        indices = np.arange(len(data))\n",
        "        if train:\n",
        "            np.random.shuffle(indices)\n",
        "\n",
        "        for start in range(0, len(data), batch_size):\n",
        "            end = min(start + batch_size, len(data))\n",
        "            batch_indices = indices[start:end]\n",
        "            batch_data = data[batch_indices]\n",
        "            batch_labels = labels[batch_indices]\n",
        "\n",
        "            if train:\n",
        "                yield datagen.flow(batch_data, batch_labels, batch_size=batch_size, shuffle=False)\n",
        "            else:\n",
        "                yield batch_data, batch_labels\n",
        "\n",
        "# Create the VGG16-based 3D model\n",
        "num_classes = len(np.unique(train_labels))\n",
        "\n",
        "def create_model(learning_rate, num_channels):\n",
        "    input_shape = (width, height, depth, num_channels)\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)  # Use VGG16\n",
        "    model = Sequential()\n",
        "    model.add(base_model)\n",
        "    model.add(GlobalAveragePooling3D())\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    optimizer = Adam(lr=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Rest of the code remains the same\n",
        "# Define a function to evaluate the model's fitness\n",
        "def evaluate_model(individual):\n",
        "    learning_rate = individual[0]\n",
        "    num_channels = train_data.shape[4]  # Get the number of channels from the training data\n",
        "    model = create_model(learning_rate, num_channels)\n",
        "\n",
        "    # Define data generators for this epoch\n",
        "    train_generator = data_generator(train_data, train_labels, batch_size, train=True)\n",
        "    val_generator = data_generator(val_data, val_labels, batch_size, train=False)\n",
        "\n",
        "    # Train the 3D model with callbacks for this epoch\n",
        "    model.fit(train_generator,\n",
        "              epochs=num_epochs,\n",
        "              validation_data=val_generator,\n",
        "              callbacks=[LearningRateScheduler(lr_schedule), early_stopping],\n",
        "              verbose=0)\n",
        "\n",
        "    # Evaluate the model on the validation data for this epoch\n",
        "    _, val_accuracy = model.evaluate(val_generator, verbose=0)\n",
        "\n",
        "    # Minimize the negative validation accuracy\n",
        "    return -val_accuracy\n",
        "\n",
        "# Profile the code using TensorFlow Profiler\n",
        "from tensorflow.python.profiler import profiler_client\n",
        "\n",
        "def profile_code():\n",
        "    with profiler_client.trace('/content/drive/MyDrive/profile_output'):\n",
        "        # Your existing code here\n",
        "        pass\n",
        "\n",
        "profile_code()\n",
        "\n",
        "# Create the DEAP toolbox\n",
        "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"attr_float\", np.random.uniform, 1e-5, 1e-3)  # Learning rate range\n",
        "toolbox.register(\"individual\", tools.initCycle, creator.Individual, (toolbox.attr_float,), n=1)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "\n",
        "# Define the genetic operators\n",
        "toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
        "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.1, indpb=0.1)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "\n",
        "# Create an initial population of 10 individuals\n",
        "population = toolbox.population(n=6)\n",
        "\n",
        "# Create the hall of fame to store the best individual\n",
        "hall_of_fame = tools.HallOfFame(1)\n",
        "\n",
        "# Create the statistics object\n",
        "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
        "stats.register(\"max\", np.max)\n",
        "stats.register(\"avg\", np.mean)\n",
        "\n",
        "# Run the genetic algorithm\n",
        "NGEN = 8  # Number of generations\n",
        "CXPB = 0.7  # Crossover probability\n",
        "MUTPB = 0.2  # Mutation probability\n",
        "\n",
        "# Create a logbook to keep track of the evolution\n",
        "logbook = tools.Logbook()\n",
        "logbook.header = [\"gen\", \"evals\", \"max\", \"avg\"]\n",
        "\n",
        "for gen in range(NGEN):\n",
        "    # Evaluate the entire population\n",
        "    fitnesses = map(evaluate_model, population)\n",
        "    for ind, fit in zip(population, fitnesses):\n",
        "        ind.fitness.values = fit\n",
        "\n",
        "    # Update the hall of fame with the best individual\n",
        "    hall_of_fame.update(population)\n",
        "\n",
        "    # Clone the population\n",
        "    offspring = list(map(toolbox.clone, population))\n",
        "\n",
        "    # Apply crossover and mutation on the offspring\n",
        "    for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
        "        if np.random.rand() < CXPB:\n",
        "            toolbox.mate(child1, child2)\n",
        "            del child1.fitness.values\n",
        "            del child2.fitness.values\n",
        "\n",
        "    for mutant in offspring:\n",
        "        if np.random.rand() < MUTPB:\n",
        "            toolbox.mutate(mutant)\n",
        "            del mutant.fitness.values\n",
        "\n",
        "    # Evaluate the offspring\n",
        "    fitnesses = map(evaluate_model, offspring)\n",
        "    for ind, fit in zip(offspring, fitnesses):\n",
        "        ind.fitness.values = fit\n",
        "\n",
        "    # Replace the old population by the offspring\n",
        "    population[:] = offspring\n",
        "\n",
        "    # Record statistics\n",
        "    record = stats.compile(population)\n",
        "    logbook.record(gen=gen, evals=len(population), **record)\n",
        "    print(logbook.stream)\n",
        "\n",
        "# Get the best individual and its fitness value from the hall of fame\n",
        "best_ind = hall_of_fame[0]\n",
        "best_fitness = best_ind.fitness.values[0]\n",
        "\n",
        "# Print the best individual and its fitness value\n",
        "print(\"Best Individual:\")\n",
        "print(\"Learning Rate =\", best_ind[0])\n",
        "print(\"Best Fitness =\", best_fitness)\n",
        "\n",
        "# Create and save the best model with the optimized learning rate\n",
        "best_learning_rate = best_ind[0]\n",
        "num_channels = train_data.shape[4]  # Get the number of channels from the training data\n",
        "best_model = create_model(best_learning_rate, num_channels)\n",
        "\n",
        "# Train the best model on the entire dataset using generators\n",
        "best_train_generator = data_generator(train_data, train_labels, batch_size)\n",
        "best_val_generator = data_generator(val_data, val_labels, batch_size, train=False)\n",
        "\n",
        "best_model.fit(best_train_generator, epochs=num_epochs, verbose=1)\n",
        "best_model.save(os.path.join(output_folder, \"Best_VGG16_model_3d.h5\"))\n",
        "\n",
        "# Evaluate the best model on the validation data\n",
        "val_loss, val_accuracy = best_model.evaluate(best_val_generator, verbose=0)\n",
        "\n",
        "# Print validation loss and accuracy of the best model\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Clear any unnecessary variables to release memory\n",
        "del data\n",
        "del labels\n",
        "del train_data\n",
        "del val_data\n"
      ]
    }
  ]
}